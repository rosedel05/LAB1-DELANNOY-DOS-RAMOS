{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebf700c-86f4-4ae2-afc6-37f1769178a6",
   "metadata": {},
   "source": [
    "# Lab 1 : Linear Regression\n",
    "\n",
    "## G3 SDI - Machine Learning\n",
    "\n",
    "In this lab, we are going to implement linear regression and ridge regression on a medical data example. The data come from a medical study (Stamey et al., 1989), whose goal was to predict the level of prostate-specific antigen (`lpsa`) from some clinical measurements. These clinical exams are carried out before a possible prostatectomy.\n",
    "\n",
    "The measurements are log cancer volume `lcavol`, log prostate weight `lweight`, age of the patient `age`, log of benign prostatic hyperplasia amount `lbph`, seminal vesicle invasion `svi`, log of capsular penetration `lcp`, Gleason score `gleason`, and percent of Gleason scores 4 or 5 `pgg45`. The variable `svi` is binary, `gleason` is ordinal, others are quantitative.\n",
    "\n",
    "### Instructions\n",
    "* Rename your notebook with your surnames as `lab1_Name1_Name2.ipynb`, and include your names in the notebook.\n",
    "* Your code, and its output, must be commented !\n",
    "* Please upload your notebook on Moodle in the dedicated section before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33a552-5dae-4ba0-a241-bd40148c260c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "Report written by [name1], [name2], date.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c0b0f5-56ec-4903-bb06-a006b81a99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import usual libraries\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9ecdf-ff04-492c-a9c3-9e35e6617c20",
   "metadata": {},
   "source": [
    "### Part 1 - Linear regression\n",
    "\n",
    "In this first part, we focus on using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db2d14-b820-4750-9c99-3699da7cd741",
   "metadata": {},
   "source": [
    "**Q1.** Load the data from the `.npy` files included in the archive (use `np.load`). How many examples are there ? How many features ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4d0e14-3f63-41b5-83bb-ed9917c15e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples : 97\n",
      "Nombre de caractéristiques : 8\n",
      "Dimension de y : (97,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('.')\n",
    "X = np.load(data_dir / 'data_X.npy')\n",
    "y = np.load(data_dir / 'data_y.npy')\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "feature_names = [f'X{i}' for i in range(n_features)]\n",
    "\n",
    "print(f\"Nombre d'exemples : {n_samples}\")\n",
    "print(f\"Nombre de caractéristiques : {n_features}\")\n",
    "print(f\"Dimension de y : {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84819978-237f-4663-8e85-b02a377a3c14",
   "metadata": {},
   "source": [
    "**Q2.** Check whether there are some missing entries in the dataset (both in X and y). Use `np.isnan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967a1c9f-abc9-4788-a984-f1da756dd7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes dans X : 0\n",
      "Valeurs manquantes dans y : 0\n"
     ]
    }
   ],
   "source": [
    "missing_in_X = np.isnan(X)\n",
    "missing_in_y = np.isnan(y)\n",
    "\n",
    "total_missing_X = int(missing_in_X.sum())\n",
    "total_missing_y = int(missing_in_y.sum())\n",
    "\n",
    "print(f\"Valeurs manquantes dans X : {total_missing_X}\")\n",
    "print(f\"Valeurs manquantes dans y : {total_missing_y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a08ba-f93f-4c42-8c89-8372ced589a1",
   "metadata": {},
   "source": [
    "**Q3.** Divide the dataset into a training set (80%) and a test set (20%), using `train_test_split` with `random_state = 0` (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6c33af-9194-443b-bc56-f8bfefceb579",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m(\n\u001b[32m      2\u001b[39m     X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m )\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTaille du jeu d\u001b[39m\u001b[33m'\u001b[39m\u001b[33mentraînement : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exemples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTaille du jeu de test : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exemples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "print(f\"Taille du jeu d'entraînement : {X_train.shape[0]} exemples\")\n",
    "print(f\"Taille du jeu de test : {X_test.shape[0]} exemples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4ac5c-d4f5-4f96-92ff-857308ba63d1",
   "metadata": {},
   "source": [
    "**Q4.** Standardize the training set, and apply the same operation to the test set. Use `StandardScaler` (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)). Recall what standardization means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef7e3d-e430-423c-b405-405a8d2878e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "train_means = X_train_std.mean(axis=0)\n",
    "train_stds = X_train_std.std(axis=0, ddof=0)\n",
    "\n",
    "print(f\"Moyenne résiduelle (train) : {np.round(train_means, 4)}\")\n",
    "print(f\"Écart-type résiduel (train) : {np.round(train_stds, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a525c1-b82f-48cd-a203-500eafa19153",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "La standardisation recentre et réduit chaque variable explicative : les moyennes résiduelles sont proches de 0 et les écarts-types proches de 1. Cela garantit que toutes les caractéristiques sont comparables en priorité et évite qu'une variable numériquement dominante n'oriente l'apprentissage.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd735cca-aa9c-4907-8620-63e5ae450132",
   "metadata": {},
   "source": [
    "**Q5.** Compute the auto-covariance matrix from the training set, and display it (you might want to use `plt.imshow`). What can we learn from this ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b96d5-98db-4b47-b008-91d6066236c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(X_train_std, rowvar=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(cov_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(feature_names)))\n",
    "ax.set_yticks(range(len(feature_names)))\n",
    "ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(feature_names)\n",
    "ax.set_title('Matrice de covariance (données standardisées)')\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06352edf-7199-4cc7-9137-1f357e9167fd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "On observe plusieurs corrélations fortes : par exemple, X0 est fortement corrélé à X5 et X4, tandis que X6 et X7 évoluent presque de concert. Ces dépendances laissent penser que les variables ne sont pas orthogonales, ce qui peut rendre la régression linéaire sensible au sur-apprentissage et justifie l'intérêt d'une régularisation.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea3dbd-24a6-47b1-b068-7cf62c8b1035",
   "metadata": {},
   "source": [
    "**Q6.** We are now going to train the linear regression model using scikit-learn (check the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)). Use the `.fit` method on the training set. Retrieve the coefficients obtained by scikit-learn using the attributes `.intercept_` and `.coef_`, and check that it corresponds to the closed-form solution from the lecture (you might want to use `np.hstack` to concatenate X with a column of ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd7a79-06f3-4aca-8f99-95f386d82a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_std, y_train)\n",
    "\n",
    "print(f\"Intercept : {lin_reg.intercept_:.4f}\")\n",
    "for name, coef in zip(feature_names, lin_reg.coef_):\n",
    "    print(f\"{name:>4s} : {coef:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee30d0-b308-45b3-a066-b193c2706622",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "Les coefficients les plus marquants sont associés à X0, X4 et X5 : ils indiquent que ces variables contribuent le plus aux variations de la cible (positivement pour X0 et X4, négativement pour X5). Les poids négatifs suggèrent une relation inverse avec la variable réponse. La présence de coefficients proches en valeur absolue pour plusieurs variables confirme la corrélation observée précédemment.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f29c19-841a-494f-b3ba-7547b341ac52",
   "metadata": {},
   "source": [
    "**Q7.** Obtain the model predictions on the test set using the `.predict` method. Then compute the MSE and the MAE (you may want to use the functions below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599f8b1-d957-4712-8606-1b3248020c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lin_reg.predict(X_train_std)\n",
    "y_pred_test = lin_reg.predict(X_test_std)\n",
    "\n",
    "lin_train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "lin_test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "lin_train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "lin_test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"MSE entraînement : {lin_train_mse:.4f}\")\n",
    "print(f\"MSE test : {lin_test_mse:.4f}\")\n",
    "print(f\"MAE entraînement : {lin_train_mae:.4f}\")\n",
    "print(f\"MAE test : {lin_test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41eade5-9d64-4277-83dc-f31e3d9d325f",
   "metadata": {},
   "source": [
    "### Part 2 - Ridge regression\n",
    "\n",
    "In this second part, we now turn to ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a40b9-bcf3-47cd-8e2b-64bb3ba65209",
   "metadata": {},
   "source": [
    "**Q1.** Fit the ridge regression model (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)) with $\\lambda = 1$, using the `.fit` method on the training set. Again, retrieve the coefficients, and check that they match with the closed-form solution from the lecture. How do they differ from the ones obtained with linear regression ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c074f0-2779-448f-a9db-eedc9d56d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_alpha_1 = Ridge(alpha=1.0)\n",
    "ridge_alpha_1.fit(X_train_std, y_train)\n",
    "\n",
    "print(f\"Intercept : {ridge_alpha_1.intercept_:.4f}\")\n",
    "for name, coef in zip(feature_names, ridge_alpha_1.coef_):\n",
    "    print(f\"{name:>4s} : {coef:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc8f46-43fa-401d-9b48-c26a589fde8c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "Le ridge réduit légèrement la magnitude des poids par rapport à la régression non régularisée, en particulier pour X5 et X2. L'effet de shrinkage reste modéré avec $\\lambda = 1$, signe que la pénalisation quadratique agit déjà comme un garde-fou face aux corrélations élevées.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c06fc-d4f4-4543-af2f-2d28b6d48803",
   "metadata": {},
   "source": [
    "**Q2.** Obtain the model predictions on the test set using the `.predict` method, then compute the MSE and the MAE. Do we get better or worse predictions than before ? Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916902e-7ed5-42df-80e7-e60575b53fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train_pred = ridge_alpha_1.predict(X_train_std)\n",
    "ridge_test_pred = ridge_alpha_1.predict(X_test_std)\n",
    "\n",
    "ridge_train_mse = mean_squared_error(y_train, ridge_train_pred)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_test_pred)\n",
    "ridge_train_mae = mean_absolute_error(y_train, ridge_train_pred)\n",
    "ridge_test_mae = mean_absolute_error(y_test, ridge_test_pred)\n",
    "\n",
    "print(f\"MSE entraînement : {ridge_train_mse:.4f}\")\n",
    "print(f\"MSE test : {ridge_test_mse:.4f}\")\n",
    "print(f\"MAE entraînement : {ridge_train_mae:.4f}\")\n",
    "print(f\"MAE test : {ridge_test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff0745-c7d6-4d8c-b1d7-922b7094c390",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "Les erreurs de test diminuent légèrement avec le ridge (MSE ~ 0.523 contre 0.540 et MAE ~ 0.556 contre 0.565). La régularisation améliore donc la généralisation, tout en conservant un biais faible sur l'entraînement.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa963d1e-7f4a-4d4c-adf2-46641b2ba024",
   "metadata": {},
   "source": [
    "**Q3.** We are now going to assess the impact of the regularization coefficient $\\lambda$.\n",
    "\n",
    "To do so, vary $\\lambda$ from $10^{-3}$ and $10^3$ (use `np.logspace`), and for each value of $\\lambda$, retrain the ridge regression model and keep the values of the coefficients (ignoring the intercept).\n",
    "\n",
    "Display the evolution of the coefficients w.r.t. $\\lambda$ (use a logarithmic scale for the x-axis). Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67584cb5-cc00-4e03-9ace-3f39fc22dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-3, 3, 50)\n",
    "ridge_coefs = np.zeros((len(lambdas), X_train_std.shape[1]))\n",
    "ridge_test_mse_path = []\n",
    "ridge_test_mae_path = []\n",
    "\n",
    "for idx, lam in enumerate(lambdas):\n",
    "    model = Ridge(alpha=lam)\n",
    "    model.fit(X_train_std, y_train)\n",
    "    ridge_coefs[idx] = model.coef_\n",
    "    preds = model.predict(X_test_std)\n",
    "    ridge_test_mse_path.append(mean_squared_error(y_test, preds))\n",
    "    ridge_test_mae_path.append(mean_absolute_error(y_test, preds))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "for j, name in enumerate(feature_names):\n",
    "    ax.plot(lambdas, ridge_coefs[:, j], label=name)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(r\"$\\lambda$\")\n",
    "ax.set_ylabel('Coefficient')\n",
    "ax.set_title('Évolution des coefficients du ridge')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90a701-e6ba-4fdf-b69c-918e16b45d7f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "Quand $\\lambda$ augmente, les coefficients sont progressivement contraints vers 0, avec un alignement visible pour les variables corrélées (X0/X5, X6/X7). Le shrinkage stabilise donc la solution et limite l'influence des caractéristiques moins informatives.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2367f4-f5d5-43ba-be06-ec8f0fca2df0",
   "metadata": {},
   "source": [
    "**Q4.** Now remains the question of choosing the optimal $\\lambda$. We are going to select it with a 5-fold cross-validation.\n",
    "\n",
    "Display the evolution of the cross-validated MSE w.r.t. $\\lambda$ (use again a logarithmic scale for the x-axis), and display the best $\\lambda$ with a `plt.axvline`.\n",
    "\n",
    "Now retrain the ridge regression model with the selected $\\lambda$, and assess its performance in terms of MSE and MAE. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70367a-2310-482f-8cf6-85daaa2d6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "cv_mse = []\n",
    "for lam in lambdas:\n",
    "    fold_mses = []\n",
    "    for train_index, val_index in kf.split(X_train_std):\n",
    "        X_tr, X_val = X_train_std[train_index], X_train_std[val_index]\n",
    "        y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        ridge_cv = Ridge(alpha=lam)\n",
    "        ridge_cv.fit(X_tr, y_tr)\n",
    "        fold_mses.append(mean_squared_error(y_val, ridge_cv.predict(X_val)))\n",
    "    cv_mse.append(np.mean(fold_mses))\n",
    "\n",
    "cv_mse = np.array(cv_mse)\n",
    "best_idx = int(np.argmin(cv_mse))\n",
    "best_lambda = lambdas[best_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(lambdas, cv_mse, marker='o')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(r\"$\\lambda$\")\n",
    "ax.set_ylabel('MSE moyenne (validation)')\n",
    "ax.set_title('Sélection de $\\lambda$ par validation croisée')\n",
    "ax.axvline(best_lambda, color='red', linestyle='--', label=f\"$\\lambda^* = {best_lambda:.3f}$\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "ridge_best = Ridge(alpha=best_lambda)\n",
    "ridge_best.fit(X_train_std, y_train)\n",
    "ridge_best_test_pred = ridge_best.predict(X_test_std)\n",
    "\n",
    "ridge_best_mse = mean_squared_error(y_test, ridge_best_test_pred)\n",
    "ridge_best_mae = mean_absolute_error(y_test, ridge_best_test_pred)\n",
    "\n",
    "print(f\"Meilleur lambda (CV) : {best_lambda:.3f}\")\n",
    "print(f\"MSE de test : {ridge_best_mse:.4f}\")\n",
    "print(f\"MAE de test : {ridge_best_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3988334",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "La validation croisée retient $\\lambda^* \\approx 0.869$. Le modèle régularisé obtenu atteint une MSE de test d'environ 0.525 et une MAE de 0.557, légèrement meilleures que la régression linéaire simple. L'écart modeste confirme que le jeu n'est pas fortement bruité, mais la régularisation apporte un supplément de stabilité grâce à la pénalisation des coefficients corrélés.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693bc54d-961e-451f-8863-5bf83b7f72e3",
   "metadata": {},
   "source": [
    "### Part 3 (Bonus) - LASSO\n",
    "\n",
    "Display the same kind of plots as in Part 2, but using LASSO regression instead of ridge regression (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html). In particular, comment on the following points :\n",
    "* Do the regression coefficients evolve in the same way as ridge regression ? What kind of solutions do we obtain ?\n",
    "* Do we get the same optimal lambda ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061e7d3-5499-4692-bc5a-d0787f7a9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_lambdas = np.logspace(-3, 1, 40)\n",
    "lasso_coefs = np.zeros((len(lasso_lambdas), X_train_std.shape[1]))\n",
    "lasso_cv_mse = []\n",
    "\n",
    "for idx, lam in enumerate(lasso_lambdas):\n",
    "    lasso_model = Lasso(alpha=lam, max_iter=10000)\n",
    "    lasso_model.fit(X_train_std, y_train)\n",
    "    lasso_coefs[idx] = lasso_model.coef_\n",
    "\n",
    "    fold_mses = []\n",
    "    kf_lasso = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_idx, val_idx in kf_lasso.split(X_train_std):\n",
    "        X_tr, X_val = X_train_std[train_idx], X_train_std[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        cv_model = Lasso(alpha=lam, max_iter=10000)\n",
    "        cv_model.fit(X_tr, y_tr)\n",
    "        fold_mses.append(mean_squared_error(y_val, cv_model.predict(X_val)))\n",
    "    lasso_cv_mse.append(np.mean(fold_mses))\n",
    "\n",
    "lasso_cv_mse = np.array(lasso_cv_mse)\n",
    "best_lasso_idx = int(np.argmin(lasso_cv_mse))\n",
    "best_lasso_lambda = lasso_lambdas[best_lasso_idx]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for j, name in enumerate(feature_names):\n",
    "    axes[0].plot(lasso_lambdas, lasso_coefs[:, j], label=name)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel(r\"$\\lambda$\")\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('Trajectoires des coefficients (LASSO)')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "axes[1].plot(lasso_lambdas, lasso_cv_mse, marker='o')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel(r\"$\\lambda$\")\n",
    "axes[1].set_ylabel('MSE moyenne (validation)')\n",
    "axes[1].set_title('Validation croisée du LASSO')\n",
    "axes[1].axvline(best_lasso_lambda, color='red', linestyle='--', label=f\"$\\lambda^* = {best_lasso_lambda:.3f}$\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "lasso_best = Lasso(alpha=best_lasso_lambda, max_iter=10000)\n",
    "lasso_best.fit(X_train_std, y_train)\n",
    "lasso_test_pred = lasso_best.predict(X_test_std)\n",
    "\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_test_pred)\n",
    "lasso_test_mae = mean_absolute_error(y_test, lasso_test_pred)\n",
    "\n",
    "print(f\"Meilleur lambda (LASSO) : {best_lasso_lambda:.3f}\")\n",
    "print(f\"MSE de test : {lasso_test_mse:.4f}\")\n",
    "print(f\"MAE de test : {lasso_test_mae:.4f}\")\n",
    "print(f\"Nombre de coefficients nuls : {(lasso_best.coef_ == 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bca27-900a-4263-99f1-47dfedf6b962",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255, 255, 0, 0.15); padding: 8px;\">\n",
    "Le LASSO sélectionne un $\\lambda$ minimal (~ 0.001), signe que la parcimonie n'apporte que peu d'avantages ici : tous les coefficients restent non nuls et les performances de test (MSE ~ 0.537, MAE ~ 0.563) s'alignent sur celles de la régression linéaire. Dans ce jeu où toutes les variables portent de l'information corrélée, la régularisation L2 du ridge semble plus pertinente pour stabiliser les poids sans les annuler brutalement.\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp1opti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
